---

title: sagemaker-tensorflow-distributed-horovod
created: 2025-08-18
modified: 2025-08-18
tags:
- service/sagemaker
- framework/tensorflow
- technique/distributed-training
- framework/horovod
- constraint/multi-gpu
aliases: ["distributed-tf", "sagemaker-horovod", "multi-gpu-training"]

---

# SageMakerì—ì„œ TensorFlow ë¶„ì‚° í›ˆë ¨ì„ ìœ„í•œ Horovod í™œìš©

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

ëŒ€ìš©ëŸ‰ ì´ë¯¸ì§€ ë°ì´í„°ì™€ ë³µì¡í•œ ëª¨ë¸ë¡œ ì¸í•´ ë‹¨ì¼ GPU í›ˆë ¨ì´ ë¶ˆì¶©ë¶„í•œ ê²½ìš° Amazon SageMakerì—ì„œ, Horovod ë¶„ì‚° í›ˆë ¨ í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ íš¨ìœ¨ì ìœ¼ë¡œ ë©€í‹° GPU í™•ì¥ì´ ê°€ëŠ¥í•˜ë‹¤.

## ğŸ“ ì„¤ëª…

### Horovodê°€ SageMaker TensorFlow ë¶„ì‚° í›ˆë ¨ì— ì í•©í•œ ì´ìœ 

HorovodëŠ” Uberì—ì„œ ê°œë°œí•œ ë¶„ì‚° ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¡œ, Ring-AllReduce ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ íš¨ìœ¨ì ì¸ ê·¸ë˜ë””ì–¸íŠ¸ ë™ê¸°í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. Amazon SageMakerì—ì„œ ê³µì‹ ì§€ì›í•˜ë©°, ê¸°ì¡´ TensorFlow ì½”ë“œì— ìµœì†Œí•œì˜ ìˆ˜ì •ë§Œìœ¼ë¡œ ë¶„ì‚° í›ˆë ¨ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. MPI(Message Passing Interface) ê¸°ë°˜ìœ¼ë¡œ ë™ì‘í•˜ì—¬ ì—¬ëŸ¬ GPUì™€ ì¸ìŠ¤í„´ìŠ¤ ê°„ í†µì‹ ì„ ìµœì í™”í•©ë‹ˆë‹¤.

### ì•„í‚¤í…ì²˜ í”Œë¡œìš°

```
ë°ì´í„° ë¡œë”© â†’ ëª¨ë¸ ë¶„ì‚° â†’ ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚° â†’ AllReduce ë™ê¸°í™” â†’ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
    â†“            â†“            â†“               â†“                â†“
ì—¬ëŸ¬ ì›Œì»¤      ê° GPUë³„      ë³‘ë ¬ ê³„ì‚°        Ring í†µì‹          ì „ì—­ ë™ê¸°í™”
ì¸ìŠ¤í„´ìŠ¤      ëª¨ë¸ ë³µì‚¬      Forward/Back    íš¨ìœ¨ì  ì§‘ê³„       ì¼ê´€ì„± ë³´ì¥
```

### Trade-offs ê³ ë ¤ì‚¬í•­

**Horovod ì¥ì **:
- Ring-AllReduceë¡œ í†µì‹  ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
- ì„ í˜•ì  í™•ì¥ì„± (ì›Œì»¤ ìˆ˜ ì¦ê°€ì— ë¹„ë¡€í•œ ì„±ëŠ¥ í–¥ìƒ)
- TensorFlow, PyTorch, MXNet ëª¨ë‘ ì§€ì›
- SageMaker ë‚´ì¥ ì§€ì›ìœ¼ë¡œ ê°„í¸í•œ ì„¤ì •

**Horovod ë‹¨ì **:
- ë™ê¸°ì‹ í›ˆë ¨ìœ¼ë¡œ ëŠë¦° ì›Œì»¤ì— ì˜í•œ ë³‘ëª© ë°œìƒ ê°€ëŠ¥
- ë„¤íŠ¸ì›Œí¬ ëŒ€ì—­í­ì— ì˜ì¡´ì 
- ì´ˆê¸° ì„¤ì • ë° ë””ë²„ê¹… ë³µì¡ì„±

**Parameter Server ë°©ì‹ ì¥ì **:
- ë¹„ë™ê¸°ì‹ ì—…ë°ì´íŠ¸ë¡œ ê°œë³„ ì›Œì»¤ ì†ë„ ì°¨ì´ í¡ìˆ˜
- Fault tolerance ê¸°ëŠ¥

**Parameter Server ë°©ì‹ ë‹¨ì **:
- ë¹„ë™ê¸° ì—…ë°ì´íŠ¸ë¡œ ì¸í•œ ìˆ˜ë ´ ë¶ˆì•ˆì •ì„±
- íŒŒë¼ë¯¸í„° ì„œë²„ ë³‘ëª© í˜„ìƒ
- ë³µì¡í•œ ì•„í‚¤í…ì²˜ êµ¬ì„± í•„ìš”

## ğŸ” ì£¼ìš”ê°œë…

### ë¶„ì‚° í›ˆë ¨ ë°©ì‹ ë¹„êµ

- **Data Parallelism**: ë™ì¼í•œ ëª¨ë¸ì„ ì—¬ëŸ¬ ì›Œì»¤ì— ë³µì‚¬í•˜ê³  ë°ì´í„°ë¥¼ ë¶„í• í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬
- **Model Parallelism**: í° ëª¨ë¸ì„ ì—¬ëŸ¬ ë””ë°”ì´ìŠ¤ì— ë¶„í• í•˜ì—¬ ë°°ì¹˜
- **Hybrid Parallelism**: ë°ì´í„° ë³‘ë ¬ê³¼ ëª¨ë¸ ë³‘ë ¬ì„ ê²°í•©í•œ ë°©ì‹

### ì‹¤ì „ ì ìš©

- **ì´ë¯¸ì§€ ë¶„ë¥˜**: ResNet, EfficientNet ë“± ëŒ€ê·œëª¨ ì´ë¯¸ì§€ë„· í›ˆë ¨
- **ìì—°ì–´ ì²˜ë¦¬**: BERT, GPT ë“± ì–¸ì–´ ëª¨ë¸ì˜ ì‚¬ì „ í›ˆë ¨
- **ì¶”ì²œ ì‹œìŠ¤í…œ**: ë”¥ëŸ¬ë‹ ê¸°ë°˜ í˜‘ì—… í•„í„°ë§ ëª¨ë¸ í›ˆë ¨

## ğŸ’» êµ¬í˜„ ì˜ˆì‹œ

```python
# Horovod ê¸°ë³¸ ì„¤ì •
import horovod.tensorflow as hvd
import tensorflow as tf

# Horovod ì´ˆê¸°í™”
hvd.init()

# GPU ì„¤ì • - ê° í”„ë¡œì„¸ìŠ¤ëŠ” í•˜ë‚˜ì˜ GPU ì‚¬ìš©
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    tf.config.experimental.set_visible_devices(gpus[hvd.local_rank()], 'GPU')

# í•™ìŠµë¥  ìŠ¤ì¼€ì¼ë§ (ì›Œì»¤ ìˆ˜ì— ë¹„ë¡€)
base_lr = 0.001
scaled_lr = base_lr * hvd.size()

optimizer = tf.keras.optimizers.Adam(scaled_lr)
optimizer = hvd.DistributedOptimizer(optimizer)

# ëª¨ë¸ ê°€ì¤‘ì¹˜ ë¸Œë¡œë“œìºìŠ¤íŠ¸
@tf.function
def training_step(images, labels, first_batch):
    with tf.GradientTape() as tape:
        predictions = model(images, training=True)
        loss = loss_fn(labels, predictions)
    
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    
    if first_batch:
        hvd.broadcast_variables(model.variables, root_rank=0)
        hvd.broadcast_variables(optimizer.variables(), root_rank=0)
    
    return loss

# SageMaker Estimator ì„¤ì •
from sagemaker.tensorflow import TensorFlow

estimator = TensorFlow(
    entry_point='train_horovod.py',
    instance_type='ml.p3.8xlarge',  # 4 GPU per instance
    instance_count=4,               # ì´ 16 GPU
    framework_version='2.11.0',
    py_version='py39',
    distribution={
        'mpi': {
            'enabled': True,
            'processes_per_host': 4  # GPU ìˆ˜ì™€ ì¼ì¹˜
        }
    },
    hyperparameters={
        'epochs': 100,
        'batch_size': 32,
        'learning_rate': 0.001
    }
)
```

## ğŸ“ ê´€ë ¨ ë¬¸ì œ

**Question:** An AI startup is developing a sophisticated image recognition model using TensorFlow on Amazon SageMaker. Due to the large volume of image data and the complexity of the model, training on a single GPU instance proves insufficient. How can the team scale their TensorFlow training job to efficiently utilize multiple GPUs within Amazon SageMaker?

**Options:**

- A) This task cannot be achieved with TensorFlow; consider using Apache MXNet for distributed training
- B) Deploy your model to multiple EC2 P3 instances, allowing SageMaker to manage distribution
- C) Write your model with the Horovod distributed training framework, supported by SageMaker
- D) Wrap your TensorFlow code in PySpark and leverage sagemaker-spark for distribution

**ì •ë‹µ: C) Write your model with the Horovod distributed training framework, supported by SageMaker**

ğŸ’¡ ì¶”ê°€ ì„¤ëª…:

- **Option A** - TensorFlowë„ ë¶„ì‚° í›ˆë ¨ì„ ì™„ì „íˆ ì§€ì›í•˜ë¯€ë¡œ í”„ë ˆì„ì›Œí¬ ë³€ê²½ì´ ë¶ˆí•„ìš”
- **Option B** - P3 ì¸ìŠ¤í„´ìŠ¤ëŠ” ì ì ˆí•˜ì§€ë§Œ ë‹¨ìˆœ ë°°í¬ë§Œìœ¼ë¡œëŠ” ë¶„ì‚° í›ˆë ¨ ìë™í™” ë¶ˆê°€
- **Option D** - PySparkëŠ” ë°ì´í„° ì²˜ë¦¬ìš©ìœ¼ë¡œ GPU ê¸°ë°˜ ë”¥ëŸ¬ë‹ ë¶„ì‚° í›ˆë ¨ê³¼ëŠ” ëª©ì ì´ ë‹¤ë¦„