---

title: neural-network-loss-oscillation-learning-rate
created: 2025-08-18
modified: 2025-08-18
tags:
- problem/optimization
- technique/gradient-descent
- issue/loss-oscillation
- parameter/learning-rate
- training/neural-network
aliases: ["loss-oscillation", "learning-rate-tuning", "gradient-descent-issues"]

---

# Neural Network Loss Function Oscillation - Learning Rate Optimization

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

ì‹ ê²½ë§ í›ˆë ¨ ì¤‘ ì†ì‹¤ í•¨ìˆ˜ê°€ ì§„ë™í•˜ëŠ” ê²½ìš°, í•™ìŠµë¥ ì´ ë„ˆë¬´ ë†’ì•„ì„œ ìµœì ê°’ ì£¼ë³€ì—ì„œ ë°œì‚°í•˜ë¯€ë¡œ í•™ìŠµë¥ ì„ ë‚®ì¶°ì•¼ í•œë‹¤.

## ğŸ“ ì„¤ëª…

### í•™ìŠµë¥ ì´ ì†ì‹¤ í•¨ìˆ˜ ì§„ë™ì— ë¯¸ì¹˜ëŠ” ì˜í–¥

í•™ìŠµë¥ (Learning Rate)ì€ ê²½ì‚¬ í•˜ê°•ë²•ì—ì„œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ì˜ í¬ê¸°ë¥¼ ê²°ì •í•˜ëŠ” í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ë„ˆë¬´ ë†’ì€ í•™ìŠµë¥ ì€ ìµœì ê°’ì„ ì°¾ëŠ” ê³¼ì •ì—ì„œ ëª©í‘œ ì§€ì ì„ "ë›°ì–´ë„˜ëŠ”" í˜„ìƒì„ ì¼ìœ¼ì¼œ ì†ì‹¤ í•¨ìˆ˜ê°€ ìœ„ì•„ë˜ë¡œ ì§„ë™í•˜ê²Œ ë§Œë“­ë‹ˆë‹¤.

### í•™ìŠµë¥ ê³¼ ìˆ˜ë ´ íŒ¨í„´

```
ë†’ì€ í•™ìŠµë¥ : Loss â†—ï¸ â†˜ï¸ â†—ï¸ â†˜ï¸ (ì§„ë™)
ì ì ˆí•œ í•™ìŠµë¥ : Loss â†˜ï¸ â†˜ï¸ â†˜ï¸ â†’ (ì•ˆì •ì  ìˆ˜ë ´)  
ë‚®ì€ í•™ìŠµë¥ : Loss â†˜ï¸ â†˜ï¸ â†˜ï¸ ... (ëŠë¦° ìˆ˜ë ´)
```

### Trade-offs ê³ ë ¤ì‚¬í•­

**ë†’ì€ í•™ìŠµë¥ ì˜ ì˜í–¥**:
- ë¹ ë¥¸ ì´ˆê¸° í•™ìŠµ ì†ë„
- ìµœì ê°’ ì£¼ë³€ì—ì„œ ì§„ë™ ë°œìƒ
- ê·¹ë‹¨ì ì¸ ê²½ìš° ë°œì‚°(divergence)

**ë‚®ì€ í•™ìŠµë¥ ì˜ ì˜í–¥**:
- ì•ˆì •ì ì¸ ìˆ˜ë ´
- ë§¤ìš° ëŠë¦° í•™ìŠµ ì†ë„
- ì§€ì—­ ìµœì ê°’ì— ê°‡í ìœ„í—˜

**ì ì‘ì  í•™ìŠµë¥  ë°©ë²•**:
- Adam, RMSprop ë“± ìë™ ì¡°ì •
- Learning Rate Scheduling ì ìš©
- í•™ìŠµ ê³¼ì •ì—ì„œ ì ì§„ì  ê°ì†Œ

## ğŸ” ì£¼ìš”ê°œë…

### í•™ìŠµë¥  ê´€ë ¨ í•µì‹¬ ê°œë…

- **Gradient Descent**: ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ìµœì í™” ì•Œê³ ë¦¬ì¦˜
- **Step Size**: í•™ìŠµë¥ ì´ ê²°ì •í•˜ëŠ” ê° ì—…ë°ì´íŠ¸ì˜ í¬ê¸°
- **Convergence**: ì†ì‹¤ í•¨ìˆ˜ê°€ ìµœì ê°’ì— ìˆ˜ë ´í•˜ëŠ” ê³¼ì •
- **Oscillation**: ìµœì ê°’ ì£¼ë³€ì—ì„œ ì†ì‹¤ì´ ìœ„ì•„ë˜ë¡œ ì§„ë™í•˜ëŠ” í˜„ìƒ

### ì‹¤ì „ ì ìš©

- **í•™ìŠµë¥  ê°ì†Œ**: í˜„ì¬ ê°’ì˜ 1/10ë¡œ ì¤„ì—¬ì„œ í…ŒìŠ¤íŠ¸
- **ì ì‘ì  ì˜µí‹°ë§ˆì´ì €**: Adam (lr=0.001), RMSprop ì‚¬ìš©
- **ìŠ¤ì¼€ì¤„ë§**: StepLR, ExponentialLRë¡œ ì ì§„ì  ê°ì†Œ
- **Early Stopping**: ì§„ë™ ê°ì§€ ì‹œ ìë™ ì¤‘ë‹¨

## ğŸ“ ê´€ë ¨ ë¬¸ì œ

**Question:** A Machine Learning Specialist splits up a large dataset into batches to train a neural network model. After some iterative tests, the Specialist noticed that the loss function is oscillating. Which is the MOST probable cause of the problem?

**Options:**

- A) The batch sizes are not small enough
- B) The learning rate is too low  
- C) The learning rate is too high
- D) The initial weight in finding the gradient descent is too high

**ì •ë‹µ: C) The learning rate is too high**

ğŸ’¡ ì¶”ê°€ ì„¤ëª…:

- **A) Batch sizes are not small enough** - ì‘ì€ ë°°ì¹˜ í¬ê¸°ëŠ” í•™ìŠµì„ ëŠë¦¬ê²Œ í•  ë¿ ì§„ë™ì˜ ì§ì ‘ì  ì›ì¸ì´ ì•„ë‹˜
- **B) The learning rate is too low** - ë‚®ì€ í•™ìŠµë¥ ì€ ëŠë¦° ìˆ˜ë ´ì„ ì•¼ê¸°í•˜ì§€ë§Œ ì§„ë™ì„ ì¼ìœ¼í‚¤ì§€ ì•ŠìŒ  
- **D) The initial weight is too high** - ì´ˆê¸° ê°€ì¤‘ì¹˜ëŠ” ì‹œì‘ì ì—ë§Œ ì˜í–¥ì„ ì£¼ë©° ì§€ì†ì ì¸ ì§„ë™ì˜ ì›ì¸ì´ ë˜ì§€ ì•ŠìŒ