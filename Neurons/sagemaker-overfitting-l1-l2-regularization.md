---
title: sagemaker-overfitting-l1-l2-regularization
created: 2025-08-15
modified: 2025-08-15
tags:
  - service/sagemaker
  - technique/regularization
  - problem/overfitting
  - parameter/dropout
  - parameter/l1-l2-regularization
aliases:
  - SageMaker 오버피팅
  - 정규화 파라미터
  - 드롭아웃 조정
  - L1L2 정규화
---

# SageMaker에서 오버피팅 해결을 위한 Regularization 파라미터 조정

## 🎯 핵심 포인트
모델 오버피팅이 발생한 경우 SageMaker 환경에서, 적절한 regularization 기법을 통해 모델 성능을 개선할 수 있다.

## 📝 설명

### Regularization이 오버피팅 해결에 적합한 이유
Regularization은 모델의 복잡도를 제어하여 훈련 데이터에 과도하게 적합되는 것을 방지하는 기법입니다. L1과 L2 regularization은 손실 함수에 페널티 항을 추가하여 가중치의 크기를 제한하고, Dropout은 훈련 중 일부 뉴런을 무작위로 비활성화하여 모델의 일반화 능력을 향상시킵니다. SageMaker에서는 이러한 기법들을 하이퍼파라미터로 쉽게 조정할 수 있어 실무에서 효과적으로 활용할 수 있습니다.

### 아키텍처 플로우
```
Training Data → Model Training → Validation
                     ↓
              Regularization 적용
              (L1/L2/Dropout)
                     ↓
              Performance 모니터링
                     ↓
              Hyperparameter 조정 → 최적 모델
```

### Trade-offs 고려사항

**Dropout 증가 장점**:
- 직접적인 오버피팅 방지 효과
- 구현이 간단하고 즉시 적용 가능
- 대부분의 딥러닝 프레임워크에서 지원

**Dropout 증가 단점**:
- 훈련 시간 증가
- 너무 높으면 언더피팅 위험
- 추론 시 성능 변화 고려 필요

**L1/L2 Regularization 장점**:
- 가중치 제어를 통한 모델 단순화
- 안정적이고 예측 가능한 성능
- Feature selection 효과 (L1의 경우)

**L1/L2 Regularization 단점**:
- 적절한 람다 값 찾기 어려움
- 과도하면 언더피팅 발생
- 도메인별 최적값 다름

## 🔍 주요개념

### 핵심 개념 비교
- **L1 Regularization (Lasso)**: 가중치의 절댓값 합에 페널티를 부과하여 일부 가중치를 0으로 만들어 feature selection 효과 제공
- **L2 Regularization (Ridge)**: 가중치의 제곱합에 페널티를 부과하여 모든 가중치를 작게 만들어 모델 복잡도 감소
- **Dropout**: 훈련 중 무작위로 뉴런을 비활성화하여 모델이 특정 뉴런에 과도하게 의존하는 것을 방지

### 실전 적용
- 이미지 분류 모델에서 validation accuracy가 training accuracy보다 현저히 낮을 때 dropout rate를 0.2에서 0.5로 증가
- 텍스트 분류에서 과도한 feature 사용으로 오버피팅 발생 시 L1 regularization 적용하여 불필요한 feature 제거
- 시계열 예측 모델에서 복잡한 패턴 학습으로 인한 오버피팅 시 L2 regularization으로 가중치 크기 제한

## 📝 관련 문제

**Question:** SageMaker에서 모델 훈련 중 오버피팅을 해결하기 위한 적절한 접근 방법은?

**Options:**
- A) Dropout rate 감소
- B) Learning rate 증가  
- C) Regularization parameter 감소
- D) 훈련 데이터 샘플 수 증가
- E) 위의 모든 것이 부적절함

**정답: E) 위의 모든 것이 부적절함**

💡 추가 설명:
- **Dropout rate 감소** - 오히려 오버피팅을 악화시킴. Dropout은 증가시켜야 함
- **Learning rate 증가** - 오버피팅 해결과 직접적 관련성이 낮으며, 훈련 불안정성 야기 가능
- **Regularization parameter 감소** - 일반적으로 오버피팅을 악화시킴. Regularization은 증가시켜야 함  
- **훈련 데이터 샘플 수 증가** - 도움이 되지만 항상 실현 가능하지 않으며, 근본적 해결책이 아님