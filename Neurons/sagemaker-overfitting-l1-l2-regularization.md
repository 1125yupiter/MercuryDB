---
title: sagemaker-regularization-overfitting-parameter-tuning
created: 2025-08-15
modified: 2025-08-15
tags:
- service/sagemaker
- technique/regularization
- problem/overfitting
- parameter/hyperparameter-tuning
- ml/model-optimization
aliases: ["regularization", "overfitting", "íŒŒë¼ë¯¸í„° ì¡°ì •", "hyperparameter tuning"]
---

# SageMakerì—ì„œ ì˜¤ë²„í”¼íŒ… í•´ê²°ì„ ìœ„í•œ Regularization íŒŒë¼ë¯¸í„° ì¡°ì •

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸
ëª¨ë¸ ì˜¤ë²„í”¼íŒ…ì´ ë°œìƒí•œ ê²½ìš° SageMaker í™˜ê²½ì—ì„œ, ì ì ˆí•œ regularization ê¸°ë²•ì„ í†µí•´ ëª¨ë¸ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆë‹¤.

## ğŸ“ ì„¤ëª…

### Regularizationì´ ì˜¤ë²„í”¼íŒ… í•´ê²°ì— ì í•©í•œ ì´ìœ 
Regularizationì€ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì œì–´í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì í•©ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. L1ê³¼ L2 regularizationì€ ì†ì‹¤ í•¨ìˆ˜ì— í˜ë„í‹° í•­ì„ ì¶”ê°€í•˜ì—¬ ê°€ì¤‘ì¹˜ì˜ í¬ê¸°ë¥¼ ì œí•œí•˜ê³ , Dropoutì€ í›ˆë ¨ ì¤‘ ì¼ë¶€ ë‰´ëŸ°ì„ ë¬´ì‘ìœ„ë¡œ ë¹„í™œì„±í™”í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. SageMakerì—ì„œëŠ” ì´ëŸ¬í•œ ê¸°ë²•ë“¤ì„ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ì‰½ê²Œ ì¡°ì •í•  ìˆ˜ ìˆì–´ ì‹¤ë¬´ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ì•„í‚¤í…ì²˜ í”Œë¡œìš°
```
Training Data â†’ Model Training â†’ Validation
                     â†“
              Regularization ì ìš©
              (L1/L2/Dropout)
                     â†“
              Performance ëª¨ë‹ˆí„°ë§
                     â†“
              Hyperparameter ì¡°ì • â†’ ìµœì  ëª¨ë¸
```

### Trade-offs ê³ ë ¤ì‚¬í•­

**Dropout ì¦ê°€ ì¥ì **:
- ì§ì ‘ì ì¸ ì˜¤ë²„í”¼íŒ… ë°©ì§€ íš¨ê³¼
- êµ¬í˜„ì´ ê°„ë‹¨í•˜ê³  ì¦‰ì‹œ ì ìš© ê°€ëŠ¥
- ëŒ€ë¶€ë¶„ì˜ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ì—ì„œ ì§€ì›

**Dropout ì¦ê°€ ë‹¨ì **:
- í›ˆë ¨ ì‹œê°„ ì¦ê°€
- ë„ˆë¬´ ë†’ìœ¼ë©´ ì–¸ë”í”¼íŒ… ìœ„í—˜
- ì¶”ë¡  ì‹œ ì„±ëŠ¥ ë³€í™” ê³ ë ¤ í•„ìš”

**L1/L2 Regularization ì¥ì **:
- ê°€ì¤‘ì¹˜ ì œì–´ë¥¼ í†µí•œ ëª¨ë¸ ë‹¨ìˆœí™”
- ì•ˆì •ì ì´ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì„±ëŠ¥
- Feature selection íš¨ê³¼ (L1ì˜ ê²½ìš°)

**L1/L2 Regularization ë‹¨ì **:
- ì ì ˆí•œ ëŒë‹¤ ê°’ ì°¾ê¸° ì–´ë ¤ì›€
- ê³¼ë„í•˜ë©´ ì–¸ë”í”¼íŒ… ë°œìƒ
- ë„ë©”ì¸ë³„ ìµœì ê°’ ë‹¤ë¦„

## ğŸ” ì£¼ìš”ê°œë…

### í•µì‹¬ ê°œë… ë¹„êµ
- **L1 Regularization (Lasso)**: ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ“ê°’ í•©ì— í˜ë„í‹°ë¥¼ ë¶€ê³¼í•˜ì—¬ ì¼ë¶€ ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ feature selection íš¨ê³¼ ì œê³µ
- **L2 Regularization (Ridge)**: ê°€ì¤‘ì¹˜ì˜ ì œê³±í•©ì— í˜ë„í‹°ë¥¼ ë¶€ê³¼í•˜ì—¬ ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ ì‘ê²Œ ë§Œë“¤ì–´ ëª¨ë¸ ë³µì¡ë„ ê°ì†Œ
- **Dropout**: í›ˆë ¨ ì¤‘ ë¬´ì‘ìœ„ë¡œ ë‰´ëŸ°ì„ ë¹„í™œì„±í™”í•˜ì—¬ ëª¨ë¸ì´ íŠ¹ì • ë‰´ëŸ°ì— ê³¼ë„í•˜ê²Œ ì˜ì¡´í•˜ëŠ” ê²ƒì„ ë°©ì§€

### ì‹¤ì „ ì ìš©
- ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì—ì„œ validation accuracyê°€ training accuracyë³´ë‹¤ í˜„ì €íˆ ë‚®ì„ ë•Œ dropout rateë¥¼ 0.2ì—ì„œ 0.5ë¡œ ì¦ê°€
- í…ìŠ¤íŠ¸ ë¶„ë¥˜ì—ì„œ ê³¼ë„í•œ feature ì‚¬ìš©ìœ¼ë¡œ ì˜¤ë²„í”¼íŒ… ë°œìƒ ì‹œ L1 regularization ì ìš©í•˜ì—¬ ë¶ˆí•„ìš”í•œ feature ì œê±°
- ì‹œê³„ì—´ ì˜ˆì¸¡ ëª¨ë¸ì—ì„œ ë³µì¡í•œ íŒ¨í„´ í•™ìŠµìœ¼ë¡œ ì¸í•œ ì˜¤ë²„í”¼íŒ… ì‹œ L2 regularizationìœ¼ë¡œ ê°€ì¤‘ì¹˜ í¬ê¸° ì œí•œ

## ğŸ“ ê´€ë ¨ ë¬¸ì œ

**Question:** A machine learning engineer notices that their SageMaker model shows excellent performance on training data but poor performance on validation data. What is the most appropriate approach to address this overfitting issue?

**Options:**
- A) Decrease the dropout rate
- B) Increase the learning rate
- C) Decrease regularization parameters
- D) Increase the number of training samples
- E) Increase dropout rate or regularization parameters

**ì •ë‹µ: E) Increase dropout rate or regularization parameters**

ğŸ’¡ ì¶”ê°€ ì„¤ëª…:
- **Decrease dropout rate** - ì˜¤íˆë ¤ ì˜¤ë²„í”¼íŒ…ì„ ì•…í™”ì‹œí‚´. Dropoutì€ ì¦ê°€ì‹œì¼œì•¼ í•¨
- **Increase learning rate** - ì˜¤ë²„í”¼íŒ… í•´ê²°ê³¼ ì§ì ‘ì  ê´€ë ¨ì„±ì´ ë‚®ìœ¼ë©°, í›ˆë ¨ ë¶ˆì•ˆì •ì„± ì•¼ê¸° ê°€ëŠ¥
- **Decrease regularization parameters** - ì¼ë°˜ì ìœ¼ë¡œ ì˜¤ë²„í”¼íŒ…ì„ ì•…í™”ì‹œí‚´. Regularizationì€ ì¦ê°€ì‹œì¼œì•¼ í•¨
- **Increase training samples** - ë„ì›€ì´ ë˜ì§€ë§Œ í•­ìƒ ì‹¤í˜„ ê°€ëŠ¥í•˜ì§€ ì•Šìœ¼ë©°, ê·¼ë³¸ì  í•´ê²°ì±…ì´ ì•„ë‹˜