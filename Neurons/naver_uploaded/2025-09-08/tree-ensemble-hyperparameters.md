---

title: tree-ensemble-hyperparameters
created: 2025-01-27
modified: 2025-01-27
tags:
- algorithm/tree-ensemble
- technique/hyperparameter
- method/supervised
- algorithm/randomforest
- algorithm/xgboost
aliases: ["tree-hyperparams", "ensemble-tuning", "tree-params"]

---

# νΈλ¦¬ κΈ°λ° μ•™μƒλΈ” λ¨λΈ ν•μ΄νΌνλΌλ―Έν„° μ™„μ „ κ°€μ΄λ“

## π― ν•µμ‹¬ ν¬μΈνΈ

νΈλ¦¬ κΈ°λ° μ•™μƒλΈ” λ¨λΈμ„ κµ¬μ„±ν•λ” κ²½μ° κ°λ³„ νΈλ¦¬μ ν•νƒμ™€ μ•™μƒλΈ” κµ¬μ΅°λ¥Ό μ μ–΄ν•λ” ν•μ΄νΌνλΌλ―Έν„°λ¥Ό ν†µν•΄ λ¨λΈμ λ³µμ΅λ„, μ„±λ¥, ν›λ ¨ μ‹κ°„μ„ μ΅°μ ν•  μ μλ‹¤.

## π“ μ„¤λ…

### νΈλ¦¬ κΈ°λ° μ•™μƒλΈ” ν•μ΄νΌνλΌλ―Έν„°μ κµ¬μ΅°μ  νΉμ§•

νΈλ¦¬ κΈ°λ° μ•™μƒλΈ” λ¨λΈμ ν•μ΄νΌνλΌλ―Έν„°λ” κ°λ³„ νΈλ¦¬μ κµ¬μ΅°λ¥Ό κ²°μ •ν•λ” μ„¤μ •κ°’λ“¤μ…λ‹λ‹¤. μ΄λ“¤μ€ λ¨λΈμ΄ λ°μ΄ν„°λ΅λ¶€ν„° ν•™μµν•λ” λ‚΄μ©(λ¶„ν•  κΈ°μ¤€κ°’, μμΈ΅κ°’)κ³Όλ” κµ¬λ³„λλ©°, μ‚¬μ©μκ°€ μ‚¬μ „μ— μ„¤μ •ν•μ—¬ νΈλ¦¬μ "λ¨μ–‘"κ³Ό "μƒμ„± λ°©μ‹"μ„ μ μ–΄ν•©λ‹λ‹¤.

ν•μ΄νΌνλΌλ―Έν„°λ” ν¬κ² μ„Έ κ°€μ§€ λ²”μ£Όλ΅ λ¶„λ¥λ©λ‹λ‹¤: (1) κ°λ³„ νΈλ¦¬ κµ¬μ΅° μ μ–΄, (2) μ•™μƒλΈ” κµ¬μ„± μ μ–΄, (3) μƒν”λ§ λ° μ •κ·ν™” μ μ–΄. κ° λ²”μ£Όμ ν•μ΄νΌνλΌλ―Έν„°λ“¤μ€ μ„λ΅ μƒνΈμ‘μ©ν•λ©° μµμΆ… λ¨λΈμ μ„±λ¥μ— μν–¥μ„ λ―ΈμΉ©λ‹λ‹¤.

### ν•μ΄νΌνλΌλ―Έν„° vs ν•™μµλ νλΌλ―Έν„°

```
ν•μ΄νΌνλΌλ―Έν„° (μ‚¬μ©μ μ„¤μ •):
β”β”€β”€ max_depth = 3         # "3μΈµκΉμ§€λ§ λ§λ“¤μ–΄λΌ"
β”β”€β”€ min_samples_split = 10 # "10κ° λ―Έλ§μ΄λ©΄ λ¶„ν•  κΈμ§€"
β””β”€β”€ n_estimators = 100    # "νΈλ¦¬ 100κ° λ§λ“¤μ–΄λΌ"

ν•™μµλ νλΌλ―Έν„° (λ¨λΈμ΄ λ°μ΄ν„°μ—μ„ λ°κ²¬):
β”β”€β”€ Root: age > 25.5      # λ¨λΈμ΄ 25.5λ¥Ό μµμ κ°’μΌλ΅ λ°κ²¬
β”β”€β”€ Left: income > 45000  # λ¨λΈμ΄ 45000μ„ μµμ κ°’μΌλ΅ λ°κ²¬  
β””β”€β”€ Right: education > 12 # λ¨λΈμ΄ 12λ¥Ό μµμ κ°’μΌλ΅ λ°κ²¬
```

### κ°λ³„ νΈλ¦¬ κµ¬μ΅° μ μ–΄ ν•μ΄νΌνλΌλ―Έν„°

**max_depth (μµλ€ νΈλ¦¬ κΉμ΄)**:
- νΈλ¦¬μ μµλ€ μΈµμλ¥Ό μ ν•ν•μ—¬ λ¨λΈ λ³µμ΅λ„λ¥Ό μ§μ ‘ μ μ–΄
- κΉμ΄ μ¦κ°€ μ‹ λ…Έλ“ μκ°€ μ§€μμ μΌλ΅ μ¦κ°€ (2^depth)
- λ„λ¬΄ κΉμΌλ©΄ μ¤λ²„ν”Όν…, λ„λ¬΄ μ–•μΌλ©΄ μ–Έλ”ν”Όν…

**min_samples_split**:
- λ…Έλ“λ¥Ό λ¶„ν• ν•κΈ° μ„ν•΄ ν•„μ”ν• μµμ† μƒν” μ
- λ†’μ„μλ΅ λ³΄μμ μΈ λ¶„ν• λ΅ μ¤λ²„ν”Όν… λ°©μ§€
- λ„λ¬΄ λ†’μΌλ©΄ μ¤‘μ”ν• ν¨ν„΄μ„ λ†“μΉ  μ μμ

**min_samples_leaf**:
- λ¦¬ν”„ λ…Έλ“κ°€ κ°€μ Έμ•Ό ν•  μµμ† μƒν” μ
- λ¨λΈ μμΈ΅μ μ•μ •μ„±μ„ λ³΄μ¥
- λ„λ¬΄ λ†’μΌλ©΄ νΈλ¦¬κ°€ λ„λ¬΄ λ‹¨μν•΄μ§

**max_features**:
- κ° λ¶„ν• μ—μ„ κ³ λ ¤ν•  μµλ€ νΉμ„± μ
- 'sqrt', 'log2', λλ” μ •μκ°’μΌλ΅ μ„¤μ •
- νΈλ¦¬ κ°„ λ‹¤μ–‘μ„± μ¦λ€λ΅ μ•™μƒλΈ” μ„±λ¥ ν–¥μƒ

### μ•™μƒλΈ” κµ¬μ„± μ μ–΄ ν•μ΄νΌνλΌλ―Έν„°

**n_estimators**:
- μƒμ„±ν•  νΈλ¦¬μ μ΄ κ°μ
- λ§μ„μλ΅ μ„±λ¥ ν–¥μƒλμ§€λ§ ν›λ ¨ μ‹κ°„ μ¦κ°€
- μν™•μ²΄κ°μ λ²•μΉ™ μ μ© (100β†’200λ³΄λ‹¤ 200β†’300μ κ°μ„ ν­μ΄ μ‘μ)

**learning_rate** (Gradient Boosting κ³„μ—΄):
- κ° νΈλ¦¬μ κΈ°μ—¬λ„λ¥Ό μ΅°μ ν•λ” κ°€μ¤‘μΉ
- λ‚®μ„μλ΅ μ²μ²ν ν•™μµν•μ§€λ§ λ” μ •ν™•
- n_estimatorsμ™€ λ°λΉ„λ΅€ κ΄€κ³„ (λ‚®μ€ learning_rate = λ§μ€ νΈλ¦¬ ν•„μ”)

### μ‹¤μ  ν•μ΄νΌνλΌλ―Έν„° μƒνΈμ‘μ© ν¨ν„΄

```
κΉμ΄-νΈλ¦¬μ κ΄€κ³„:
β”β”€β”€ κΉμ€ νΈλ¦¬(15+) + μ μ€ κ°μ(50) = λ³µμ΅ν•μ§€λ§ λ‹¤μ–‘μ„± λ¶€μ΅±
β”β”€β”€ μ–•μ€ νΈλ¦¬(3-5) + λ§μ€ κ°μ(500) = λ‹¨μν•μ§€λ§ λ‹¤μ–‘ν• μ΅°ν•©
β””β”€β”€ μ¤‘κ°„ κΉμ΄(8-12) + μ λ‹Ήν• κ°μ(100-300) = κ· ν•μ΅ν μ„±λ¥

ν•™μµλ¥ -νΈλ¦¬μ κ΄€κ³„ (Boosting):
β”β”€β”€ λ‚®μ€ ν•™μµλ¥ (0.01) + λ§μ€ νΈλ¦¬(1000) = μ²μ²ν μ •ν™•ν•κ²
β”β”€β”€ λ†’μ€ ν•™μµλ¥ (0.3) + μ μ€ νΈλ¦¬(100) = λΉ λ¥΄μ§€λ§ λ¶μ•μ •
β””β”€β”€ μ¤‘κ°„ ν•™μµλ¥ (0.1) + μ λ‹Ήν• νΈλ¦¬(300) = μ‹¤μ©μ  κ· ν•
```

## π” μ£Όμ”κ°λ…

### μ•κ³ λ¦¬μ¦λ³„ ν•µμ‹¬ ν•μ΄νΌνλΌλ―Έν„°

- **Random Forest**: n_estimators, max_depth, max_features, bootstrap
- **XGBoost**: n_estimators, max_depth, learning_rate, subsample, colsample_bytree
- **LightGBM**: n_estimators, max_depth, learning_rate, num_leaves, feature_fraction
- **CatBoost**: iterations, depth, learning_rate, l2_leaf_reg

### ν•μ΄νΌνλΌλ―Έν„° νλ‹ μ „λµ

**Grid Search μμ‹**:
```python
param_grid = {
    'max_depth': [5, 10, 15],
    'n_estimators': [100, 200, 300],
    'min_samples_split': [2, 5, 10]
}
# μ΄ 27κ°€μ§€ μ΅°ν•© (3Γ—3Γ—3)
```

**Random Search μμ‹**:
```python
param_distributions = {
    'max_depth': randint(3, 20),
    'n_estimators': randint(50, 500),
    'learning_rate': uniform(0.01, 0.3)
}
# λ¬΄μ‘μ„λ΅ 100κ° μ΅°ν•© μƒν”λ§
```

### μ‹¤μ „ μ μ©

- **κ³ μ°¨μ› λ°μ΄ν„°**: max_features='sqrt'λ΅ νΉμ„± λ¶€λ¶„μ§‘ν•© μ‚¬μ©ν•μ—¬ λ‹¤μ–‘μ„± ν™•λ³΄
- **λ¶κ· ν• λ°μ΄ν„°**: class_weight μ΅°μ •μΌλ΅ μ†μ ν΄λμ¤μ— λ” λ†’μ€ κ°€μ¤‘μΉ
- **μ‹¤μ‹κ°„ μμΈ΅**: max_depth μ ν•μΌλ΅ μμΈ΅ μ†λ„ ν–¥μƒ, λ©”λ¨λ¦¬ μ‚¬μ©λ‰ κ°μ†

## π“ κ΄€λ ¨ λ¬Έμ 

**Question:** A data scientist is tuning hyperparameters for a Random Forest classifier on a dataset with 50,000 samples and 200 features. The current model shows signs of overfitting with training accuracy of 99% and validation accuracy of 75%. Which combination of hyperparameter adjustments would be most effective in reducing overfitting?

**Options:**

- A) Increase max_depth and increase n_estimators
- B) Decrease max_depth and increase min_samples_split  
- C) Increase max_features and decrease min_samples_leaf
- D) Decrease n_estimators and increase max_features
- E) Increase learning_rate and decrease max_depth

**μ •λ‹µ: B) Decrease max_depth and increase min_samples_split**

π’΅ μ¶”κ°€ μ„¤λ…:

- **Option A** - max_depth μ¦κ°€λ” μ¤λ²„ν”Όν…μ„ μ•…ν™”μ‹ν‚΄, λ” λ³µμ΅ν• νΈλ¦¬ μƒμ„±
- **Option B** - λ‘ νλΌλ―Έν„° λ¨λ‘ λ¨λΈ λ³µμ΅λ„λ¥Ό κ°μ†μ‹μΌ μ¤λ²„ν”Όν… μ™„ν™”μ— ν¨κ³Όμ 
- **Option C** - max_features μ¦κ°€μ™€ min_samples_leaf κ°μ† λ¨λ‘ μ¤λ²„ν”Όν… μ λ°
- **Option D** - n_estimators κ°μ†λ” μ•™μƒλΈ” μ„±λ¥μ„ λ–¨μ–΄λ¨λ¦΄ μ μμ
- **Option E** - learning_rateλ” Random Forestμ— μ μ©λμ§€ μ•λ” ν•μ΄νΌνλΌλ―Έν„°