---

title: csv-parquet-glue-athena-realtime
created: 2025-08-17
modified: 2025-08-17
tags:
- service/kinesis-firehose
- service/glue-streaming
- format/csv-parquet
- constraint/realtime
- usecase/athena-analytics
aliases: ["csv-to-parquet", "firehose-limitations", "glue-streaming-etl"]

---

# AWS Glue Streaming ETLì´ CSV to Parquet ì‹¤ì‹œê°„ ë³€í™˜ì— ì í•©í•œ ì´ìœ 

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

ì˜¨í”„ë ˆë¯¸ìŠ¤ì—ì„œ ìƒì„±ë˜ëŠ” CSV íŒŒì¼ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ Parquet í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ S3ì— ì €ì¥í•˜ê³  Athenaë¡œ ë¶„ì„í•´ì•¼ í•˜ëŠ” ê²½ìš°, AWS Glue Streaming ETLì„ ì‚¬ìš©í•˜ë©´ ìµœì†Œí•œì˜ êµ¬ì„± ì˜¤ë²„í—¤ë“œë¡œ ìŠ¤í‚¤ë§ˆ ì¶”ë¡ ë¶€í„° Data Catalog ì—…ë°ì´íŠ¸ê¹Œì§€ ìë™í™”í•  ìˆ˜ ìˆë‹¤.

## ğŸ“ ì„¤ëª…

### AWS Glue Streaming ETLì´ CSV to Parquet ë³€í™˜ì— ì í•©í•œ ì´ìœ 

**Kinesis Data Firehoseì˜ í•µì‹¬ ì œì•½ì‚¬í•­**: FirehoseëŠ” **êµ¬ì¡°í™”ëœ ìŠ¤í‚¤ë§ˆ ì •ë³´ê°€ í¬í•¨ëœ ë°ì´í„°**ë§Œ ì§ì ‘ Parquetìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆë‹¤. JSONì€ ìì²´ì ìœ¼ë¡œ ë°ì´í„° íƒ€ì… ì •ë³´ë¥¼ í¬í•¨í•˜ê³  ìˆì–´ ìë™ ìŠ¤í‚¤ë§ˆ ì¶”ë¡ ì´ ê°€ëŠ¥í•˜ì§€ë§Œ, CSVëŠ” ëª¨ë“  ê°’ì´ ë¬¸ìì—´ë¡œ ì²˜ë¦¬ë˜ì–´ ë³„ë„ì˜ ìŠ¤í‚¤ë§ˆ ì •ì˜ê°€ í•„ìš”í•˜ë‹¤.

**Glue Streaming ETLì˜ ì¥ì **: ì„œë²„ë¦¬ìŠ¤ í™˜ê²½ì—ì„œ ë³µì¡í•œ ë°ì´í„° ë³€í™˜ ë¡œì§ì„ ì‹¤í–‰í•  ìˆ˜ ìˆìœ¼ë©°, CSV íŒŒì‹±ë¶€í„° ë°ì´í„° íƒ€ì… ì¶”ë¡ , Parquet ìµœì í™”ê¹Œì§€ ëª¨ë“  ê³¼ì •ì„ ìë™í™”í•œë‹¤. íŠ¹íˆ Athena ë¶„ì„ì„ ìœ„í•œ íŒŒí‹°ì…”ë‹, Data Catalog ì—…ë°ì´íŠ¸, ì¿¼ë¦¬ ìµœì í™”ê¹Œì§€ ê³ ë ¤í•œ íŒŒì´í”„ë¼ì¸ì„ êµ¬ì„±í•  ìˆ˜ ìˆë‹¤.

### ì•„í‚¤í…ì²˜ í”Œë¡œìš°

```
ì˜¨í”„ë ˆë¯¸ìŠ¤ CSV íŒŒì¼ ìƒì„±
â†“
Kinesis Data Streams (ì‹¤ì‹œê°„ ìˆ˜ì§‘)
â†“
Glue Streaming ETL
â”œâ”€â”€ CSV íŒŒì‹± ë° ìŠ¤í‚¤ë§ˆ ì¶”ë¡ 
â”œâ”€â”€ ë°ì´í„° íƒ€ì… ë³€í™˜
â”œâ”€â”€ ìë™ íŒŒí‹°ì…”ë‹ (ë…„/ì›”/ì¼)
â”œâ”€â”€ Parquet ìµœì í™” (ì••ì¶•, ì»¬ëŸ¼í˜• ì €ì¥)
â””â”€â”€ Data Catalog ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
â†“
S3 (Athena ìµœì í™”ëœ Parquet íŒŒì¼)
â†“
Amazon Athena (ê³ ì„±ëŠ¥ ì¿¼ë¦¬)
```

### Trade-offs ê³ ë ¤ì‚¬í•­

**Glue Streaming ETL ì¥ì **:
- ì„œë²„ë¦¬ìŠ¤ë¡œ ì¸í”„ë¼ ê´€ë¦¬ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”
- ë³µì¡í•œ ë°ì´í„° ë³€í™˜ ë¡œì§ ì§€ì› (CSV ìŠ¤í‚¤ë§ˆ ì¶”ë¡ )
- Athena ìµœì í™” ìë™ ì ìš© (íŒŒí‹°ì…”ë‹, Data Catalog ì—°ë™)
- Apache Spark ê¸°ë°˜ì˜ ê°•ë ¥í•œ ë³‘ë ¬ ì²˜ë¦¬ ëŠ¥ë ¥

**Glue Streaming ETL ë‹¨ì **:
- ì´ˆê¸° ì„¤ì • ì‹œ ìŠ¤í‚¤ë§ˆ ì •ì˜ í•„ìš”í•  ìˆ˜ ìˆìŒ
- Spark ê¸°ë°˜ìœ¼ë¡œ ìƒëŒ€ì ìœ¼ë¡œ ë†’ì€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©

**Kinesis Data Firehose ì¥ì **:
- ì„¤ì •ì´ ë§¤ìš° ê°„ë‹¨
- JSON to ParquetëŠ” ì™„ë²½í•˜ê²Œ ì§€ì›

**Kinesis Data Firehose ë‹¨ì **:
- CSV to Parquet ì§ì ‘ ë³€í™˜ ë¶ˆê°€ëŠ¥
- ë³µì¡í•œ ë°ì´í„° ë³€í™˜ ë¡œì§ ì œí•œì 
- Athena ìµœì í™” ê¸°ëŠ¥ ë¶€ì¡±

## ğŸ” ì£¼ìš”ê°œë…

### ë°ì´í„° í˜•ì‹ë³„ Firehose ë³€í™˜ ì§€ì›

- **JSON â†’ Parquet/ORC**: âœ… ì™„ì „ ì§€ì› (ìì²´ ìŠ¤í‚¤ë§ˆ ì •ë³´ í¬í•¨)
- **CSV â†’ Parquet/ORC**: âŒ ì§ì ‘ ë³€í™˜ ë¶ˆê°€ (ìŠ¤í‚¤ë§ˆ ì •ë³´ ë¶€ì¡±)
- **XML â†’ Parquet/ORC**: âŒ êµ¬ì¡°ê°€ ë„ˆë¬´ ë³µì¡
- **Avro â†’ Parquet/ORC**: âŒ ì§ì ‘ ì§€ì› ì•ˆí•¨

### Firehose ë³€í™˜ì˜ "ê³¨ë””ë½ìŠ¤ ì¡´"

FirehoseëŠ” **ë„ˆë¬´ ë‹¨ìˆœí•˜ì§€ë„, ë„ˆë¬´ ë³µì¡í•˜ì§€ë„ ì•Šì€** JSON í˜•ì‹ì— ìµœì í™”ë˜ì–´ ìˆë‹¤:
- **ë„ˆë¬´ ë‹¨ìˆœ (CSV)**: ë°ì´í„° íƒ€ì… ì •ë³´ ì—†ìŒ
- **ì ë‹¹í•¨ (JSON)**: êµ¬ì¡°í™” + íƒ€ì… ì •ë³´ í¬í•¨
- **ë„ˆë¬´ ë³µì¡ (XML)**: ì¤‘ì²© êµ¬ì¡°ë¡œ ì»¬ëŸ¼í˜• ë§¤í•‘ ì–´ë ¤ì›€

### ì‹¤ì „ ì ìš©

- **ì‹¤ì‹œê°„ ë¡œê·¸ ë¶„ì„**: ì• í”Œë¦¬ì¼€ì´ì…˜ ë¡œê·¸ CSVë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ Parquet ë³€í™˜í•˜ì—¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- **IoT ì„¼ì„œ ë°ì´í„°**: ì œì¡°ì—… ì„¼ì„œ CSV ë°ì´í„°ë¥¼ íŒŒí‹°ì…”ë‹í•˜ì—¬ ì‹œê³„ì—´ ë¶„ì„ ìµœì í™”
- **ê¸ˆìœµ ê±°ë˜ ë°ì´í„°**: ì‹¤ì‹œê°„ ê±°ë˜ CSVë¥¼ Parquetìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì´ìƒ ê±°ë˜ íŒ¨í„´ ë¶„ì„

## ğŸ“ ê´€ë ¨ ë¬¸ì œ

**Question:** A Data Engineer is designing a solution for customer data analysis using Amazon Athena. An on-premises application produces the data as CSV files in near real-time. The Engineer needs to convert the data to Apache Parquet format before saving it on an Amazon S3 bucket. Which method provides the LEAST configuration overhead?

**Options:**

- A) Use Amazon Kinesis Data Streams to consume customer data. Create a streaming ETL job in AWS Glue to convert data into Apache Parquet.
- B) Configure an Amazon EMR cluster with Apache Spark Structured Streaming to consume and transform the customer data into Apache Parquet.
- C) Configure an Amazon EC2 instance with Apache Kafka to consume the customer data. Export the data to the S3 bucket in Parquet format with Kafka Connect S3 sink connector.
- D) Use Amazon Kinesis Data Streams to ingest customer data and configure a Firehose stream as a consumer to convert the data into Apache Parquet.

**ì •ë‹µ: A) Use Amazon Kinesis Data Streams to consume customer data. Create a streaming ETL job in AWS Glue to convert data into Apache Parquet.**

ğŸ’¡ ì¶”ê°€ ì„¤ëª…:

- **ì˜µì…˜ B (EMR í´ëŸ¬ìŠ¤í„°)** - í´ëŸ¬ìŠ¤í„° ì„¤ì •, ê´€ë¦¬, í™•ì¥ì„± êµ¬ì„± ë“± ë§ì€ ì¸í”„ë¼ ê´€ë¦¬ ì˜¤ë²„í—¤ë“œ ë°œìƒ
- **ì˜µì…˜ C (EC2 + Kafka)** - EC2 ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬, Kafka ì„¤ì •, Kafka Connect êµ¬ì„± ë“± ê°€ì¥ ë³µì¡í•œ ì„¤ì • ì‘ì—… í•„ìš”
- **ì˜µì…˜ D (Firehose)** - CSVë¥¼ ì§ì ‘ Parquetìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ëŠ¥ ì œí•œì , JSONë§Œ ì§ì ‘ ë³€í™˜ ì§€ì›
- **í•µì‹¬ ì´ìœ **: Athena ë¶„ì„ì´ ìµœì¢… ëª©ì ì´ë¯€ë¡œ íŒŒí‹°ì…”ë‹, Data Catalog ì—°ë™, ì¿¼ë¦¬ ìµœì í™”ê¹Œì§€ ê³ ë ¤í•œ Glue Streaming ETLì´ ì „ì²´ íŒŒì´í”„ë¼ì¸ ê´€ì ì—ì„œ ê°€ì¥ íš¨ìœ¨ì 