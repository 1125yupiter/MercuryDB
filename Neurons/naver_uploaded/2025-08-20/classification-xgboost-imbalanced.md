---

title: classification-xgboost-imbalanced
created: 2025-08-17
modified: 2025-08-17
tags:
- problem/classification
- service/sagemaker
- algorithm/xgboost
- constraint/imbalanced-data
- usecase/fraud-detection
aliases: ["fraud-detection", "class-imbalance", "xgb-fraud"]

---

# XGBoostë¥¼ í™œìš©í•œ ë¶ˆê· í˜• ì‚¬ê¸° ê±°ë˜ íƒì§€ ì‹œìŠ¤í…œ

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

ê·¹ë„ë¡œ ë¶ˆê· í˜•í•œ ì‚¬ê¸° ê±°ë˜ ë°ì´í„°ì…‹(1:100 ë¹„ìœ¨)ì—ì„œ ë†’ì€ False Negative Rate(87.7%)ê°€ ë°œìƒí•œ ê²½ìš°, scale_pos_weight í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •ê³¼ AUC í‰ê°€ ì§€í‘œ ì‚¬ìš©ìœ¼ë¡œ ì‚¬ê¸° ê±°ë˜ íƒì§€ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.

## ğŸ“ ì„¤ëª…

### XGBoostê°€ ë¶ˆê· í˜• ë°ì´í„° ì‚¬ê¸° íƒì§€ì— ì í•©í•œ ì´ìœ 

XGBoostëŠ” gradient boosting ê¸°ë°˜ì˜ ì•™ìƒë¸” í•™ìŠµìœ¼ë¡œ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆìœ¼ë©°, scale_pos_weight íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆë‹¤. íŠ¹íˆ ì‚¬ê¸° ê±°ë˜ íƒì§€ì—ì„œëŠ” False Negative(ì‹¤ì œ ì‚¬ê¸°ë¥¼ ë†“ì¹˜ëŠ” ê²½ìš°)ê°€ ì§ì ‘ì ì¸ ê¸ˆì „ì  ì†ì‹¤ë¡œ ì´ì–´ì§€ë¯€ë¡œ, ì†Œìˆ˜ í´ë˜ìŠ¤ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ì¡°ì •ì´ í•„ìˆ˜ì ì´ë‹¤.

### ì•„í‚¤í…ì²˜ í”Œë¡œìš°

```
Raw Transaction Data (1:100 ratio)
        â†“
Data Preprocessing & Feature Engineering
        â†“
XGBoost Training with scale_pos_weight=100
        â†“
Model Evaluation with AUC metric
        â†“
Threshold Optimization (< 0.5)
        â†“
Production Deployment with Real-time Scoring
```

### Trade-offs ê³ ë ¤ì‚¬í•­

**XGBoost + scale_pos_weight ì¥ì **:
- í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ë‚´ì¬ì ìœ¼ë¡œ í•´ê²°
- ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ê³¼ í•´ì„ ê°€ëŠ¥ì„±
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ìœ¼ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ ë°˜ì˜ ê°€ëŠ¥
- ì‹¤ì‹œê°„ ì¶”ë¡ ì— ì í•©í•œ ì†ë„

**XGBoost + scale_pos_weight ë‹¨ì **:
- False Positive ì¦ê°€ë¡œ ì¸í•œ ê³ ê° ë¶ˆí¸ ê°€ëŠ¥ì„±
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì˜ ë³µì¡ì„±
- ë°ì´í„° ë¶„í¬ ë³€í™”ì— ë¯¼ê°

**CNN ê¸°ë°˜ ëª¨ë¸ ì¥ì **:
- ìˆœì°¨ì  ê±°ë˜ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥
- ë³µì¡í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ë¶ˆí•„ìš”

**CNN ê¸°ë°˜ ëª¨ë¸ ë‹¨ì **:
- ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬ê°€ ë³µì¡
- í›ˆë ¨ ì‹œê°„ê³¼ ë¦¬ì†ŒìŠ¤ ìš”êµ¬ëŸ‰ ë†’ìŒ
- í•´ì„ ê°€ëŠ¥ì„± ë¶€ì¡±

## ğŸ” ì£¼ìš”ê°œë…

### í‰ê°€ ì§€í‘œ ë¹„êµ

- **Accuracy**: ì „ì²´ ì˜ˆì¸¡ì˜ ì •í™•ë„, ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ ì˜¤í•´ì˜ ì†Œì§€
- **AUC (Area Under Curve)**: ROC ê³¡ì„  í•˜ë‹¨ ë©´ì , ì„ê³„ê°’ ë³€í™”ì— ëŒ€í•œ ì¢…í•© ì„±ëŠ¥
- **Precision**: ì‚¬ê¸°ë¡œ ì˜ˆì¸¡í•œ ê²ƒ ì¤‘ ì‹¤ì œ ì‚¬ê¸° ë¹„ìœ¨
- **Recall**: ì‹¤ì œ ì‚¬ê¸° ì¤‘ ì˜¬ë°”ë¥´ê²Œ íƒì§€í•œ ë¹„ìœ¨

### ì‹¤ì „ ì ìš©

- **ê¸ˆìœµ ì‚¬ê¸° íƒì§€**: ì‹ ìš©ì¹´ë“œ ê±°ë˜, ì˜¨ë¼ì¸ ê²°ì œ ì´ìƒ íƒì§€
- **ë³´í—˜ ì²­êµ¬ ì‹¬ì‚¬**: í—ˆìœ„ ì²­êµ¬ ìë™ ìŠ¤í¬ë¦¬ë‹
- **ì´ì»¤ë¨¸ìŠ¤ ë¦¬ìŠ¤í¬ ê´€ë¦¬**: ê³„ì • ë„ìš©, ê²°ì œ ì‚¬ê¸° ë°©ì§€

## ğŸ“ ê´€ë ¨ ë¬¸ì œ

**Question:** A Machine Learning Specialist is training an XGBoost CNN-based model for detecting fraudulent transactions using Amazon SageMaker AI. The training data contains 5,000 fraudulent behaviors and 500,000 non-fraudulent behaviors. The model reaches an accuracy of 99.5% during training. When tested on the validation dataset, the model shows an accuracy of 99.1% but delivers a high false-negative rate of 87.7%. The Specialist needs to bring down the number of false-negative predictions for the model to be acceptable in production. Which combination of actions must be taken to meet the requirement? (Select TWO.)

**Options:**

- A) Increase the model complexity by specifying a larger value for the max_depth hyperparameter
- B) Alter the value of the eval_metric hyperparameter to MAP (Mean Average Precision)
- C) Adjust the balance of positive and negative weights by configuring the scale_pos_weight hyperparameter
- D) Alter the value of the eval_metric hyperparameter to Area Under The Curve (AUC)
- E) Increase the value of the rate_drop hyperparameter to reduce the overfitting of the model

**ì •ë‹µ: C, D) scale_pos_weight ì¡°ì • + AUC í‰ê°€ ì§€í‘œ ì‚¬ìš©**

ğŸ’¡ ì¶”ê°€ ì„¤ëª…:

- **Option A** - ì´ë¯¸ ì¶©ë¶„í•œ ì¼ë°˜í™” ì„±ëŠ¥(99.1% validation accuracy)ì„ ë³´ì´ë¯€ë¡œ ë³µì¡ë„ ì¦ê°€ ë¶ˆí•„ìš”
- **Option B** - MAPì€ ë­í‚¹ ì•Œê³ ë¦¬ì¦˜ í‰ê°€ìš©ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ì— ë¶€ì í•©
- **Option E** - ì˜¤ë²„í”¼íŒ… ë¬¸ì œê°€ ì•„ë‹ˆë¯€ë¡œ ì •ê·œí™” ê°•í™”ëŠ” ë¹„íš¨ê³¼ì 

### í•µì‹¬ í•´ê²° ë°©ë²•

1. **scale_pos_weight ê³„ì‚°**: negative_samples / positive_samples = 500,000 / 5,000 = 100
2. **AUC ê¸°ë°˜ í‰ê°€**: ë¶ˆê· í˜• ë°ì´í„°ì—ì„œ accuracyë³´ë‹¤ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì§€í‘œ
3. **ì„ê³„ê°’ ìµœì í™”**: ê¸°ë³¸ 0.5 ëŒ€ì‹  ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ì„ê³„ê°’ ì„¤ì •
4. **ì¶”ê°€ ê¸°ë²•**: SMOTE, ì–¸ë”ìƒ˜í”Œë§, cost-sensitive learning ê³ ë ¤