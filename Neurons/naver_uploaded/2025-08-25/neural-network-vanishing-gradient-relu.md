---

title: neural-network-vanishing-gradient-relu
created: 2025-08-18
modified: 2025-08-18
tags:
- problem/vanishing-gradient
- technique/relu-activation
- method/deep-learning
- constraint/layer-depth
- solution/gradient-preservation
aliases: ["vanishing-gradient", "relu-solution", "deep-network-training"]

---

# ì‹ ê²½ë§ ë³µì¡ë„ ì¦ê°€ ì‹œ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œì™€ ReLU í•´ê²°ì±…

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

ì‹ ê²½ë§ì˜ ë ˆì´ì–´ ìˆ˜ë¥¼ ëŠ˜ë ¤ ë³µì¡ë„ë¥¼ ì¦ê°€ì‹œí‚¨ ê²½ìš° ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¡œ ì¸í•´ ìˆ˜ë ´ì´ ì–´ë ¤ì›Œì§€ëŠ”ë°, ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ìš¸ê¸°ë¥¼ ë³´ì¡´í•  ìˆ˜ ìˆë‹¤.

## ğŸ“ ì„¤ëª…

### ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œê°€ ë³µì¡ë„ ì¦ê°€ì™€ ì—°ê´€ëœ ì´ìœ 

ì‹ ê²½ë§ì˜ ë³µì¡ë„ ì¦ê°€ëŠ” ì£¼ë¡œ ë ˆì´ì–´ ìˆ˜(ê¹Šì´) ì¦ê°€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì—­ì „íŒŒ ê³¼ì •ì—ì„œ ê¸°ìš¸ê¸°ëŠ” ê° ë ˆì´ì–´ë¥¼ ê±°ì¹˜ë©° í™œì„±í™” í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ì™€ ê³±í•´ì§€ëŠ”ë°, ì „í†µì ì¸ í™œì„±í™” í•¨ìˆ˜ë“¤(Sigmoid, Tanh)ì˜ ìµœëŒ€ ê¸°ìš¸ê¸°ê°€ ì œí•œì ì´ì–´ì„œ ë ˆì´ì–´ê°€ ê¹Šì–´ì§ˆìˆ˜ë¡ ê¸°ìš¸ê¸°ê°€ ê¸‰ê²©íˆ ê°ì†Œí•©ë‹ˆë‹¤.

### ìˆ˜í•™ì  ë©”ì»¤ë‹ˆì¦˜

#### Sigmoid í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸° íŠ¹ì„±
- ìµœëŒ€ ê¸°ìš¸ê¸°: 0.25 (x=0ì¼ ë•Œ)
- ëŒ€ë¶€ë¶„ êµ¬ê°„ì—ì„œ ê¸°ìš¸ê¸° < 0.25

#### ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œì˜ ê¸°ìš¸ê¸° ì „íŒŒ
```
5ê°œ ë ˆì´ì–´ Sigmoid ë„¤íŠ¸ì›Œí¬:
ìµœì¢… ê¸°ìš¸ê¸° = 0.25^5 = 0.0009765 (ê±°ì˜ 0ì— ìˆ˜ë ´)

vs.

5ê°œ ë ˆì´ì–´ ReLU ë„¤íŠ¸ì›Œí¬:
ìµœì¢… ê¸°ìš¸ê¸° = 1^5 = 1.0 (ê¸°ìš¸ê¸° ë³´ì¡´)
```

### ì•„í‚¤í…ì²˜ í”Œë¡œìš°

```
Input Data
    â†“
[Layer 1: ReLU] â†’ ê¸°ìš¸ê¸°: 1.0
    â†“
[Layer 2: ReLU] â†’ ê¸°ìš¸ê¸°: 1.0
    â†“
[Layer 3: ReLU] â†’ ê¸°ìš¸ê¸°: 1.0
    â†“
[Layer 4: ReLU] â†’ ê¸°ìš¸ê¸°: 1.0
    â†“
[Output Layer] â†’ ê¸°ìš¸ê¸°: 1.0
    â†“
Loss Calculation
    â†“
Backpropagation (ê¸°ìš¸ê¸° ë³´ì¡´)
```

### Trade-offs ê³ ë ¤ì‚¬í•­

**ReLU í™œì„±í™” í•¨ìˆ˜ ì¥ì **:
- ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²° (x > 0ì¼ ë•Œ ê¸°ìš¸ê¸° = 1)
- ê³„ì‚° íš¨ìœ¨ì„± (ë‹¨ìˆœí•œ max(0, x) ì—°ì‚°)
- í¬ì†Œ í™œì„±í™” (ìŒìˆ˜ ì…ë ¥ì—ì„œ 0 ì¶œë ¥)
- ê¹Šì€ ë„¤íŠ¸ì›Œí¬ í•™ìŠµ ê°€ëŠ¥

**ReLU í™œì„±í™” í•¨ìˆ˜ ë‹¨ì **:
- Dying ReLU ë¬¸ì œ (ìŒìˆ˜ ì˜ì—­ì—ì„œ ê¸°ìš¸ê¸° = 0)
- ì¶œë ¥ ë²”ìœ„ ì œí•œ ì—†ìŒ (0 ~ âˆ)
- ìŒìˆ˜ ì •ë³´ ì†ì‹¤

**Sigmoid/Tanh í•¨ìˆ˜ ì¥ì **:
- ì¶œë ¥ ë²”ìœ„ ì œí•œ (í™•ë¥ ì  í•´ì„ ê°€ëŠ¥)
- ë§¤ë„ëŸ¬ìš´ ê¸°ìš¸ê¸°

**Sigmoid/Tanh í•¨ìˆ˜ ë‹¨ì **:
- ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ (ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì¹˜ëª…ì )
- ê³„ì‚° ë¹„ìš© ë†’ìŒ
- Gradient Saturation ë¬¸ì œ

## ğŸ” ì£¼ìš”ê°œë…

### í™œì„±í™” í•¨ìˆ˜ë³„ ê¸°ìš¸ê¸° íŠ¹ì„±

- **ReLU**: f(x) = max(0, x), f'(x) = 1 (if x > 0), 0 (if x â‰¤ 0)
- **Sigmoid**: f(x) = 1/(1+e^(-x)), f'(x) = f(x)(1-f(x)), ìµœëŒ€ê°’ 0.25
- **Tanh**: f(x) = tanh(x), f'(x) = 1-tanhÂ²(x), ìµœëŒ€ê°’ 1

### ì‹¤ì „ ì ìš©

- **ì´ë¯¸ì§€ ë¶„ë¥˜**: CNNì—ì„œ ê¹Šì€ ë ˆì´ì–´ êµ¬ì„± ì‹œ ReLU í•„ìˆ˜
- **ìì—°ì–´ ì²˜ë¦¬**: Transformer ëª¨ë¸ì˜ ê¹Šì€ í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬
- **ì‹œê³„ì—´ ì˜ˆì¸¡**: ë³µì¡í•œ íŒ¨í„´ í•™ìŠµì„ ìœ„í•œ ê¹Šì€ LSTM/GRU ë„¤íŠ¸ì›Œí¬

## ğŸ“ ê´€ë ¨ ë¬¸ì œ

**Question:** A data science team at an energy analytics company is using neural networks to improve prediction accuracy for electricity consumption. Initially underperforming, they increased model complexity by adding more layers. However, after modification, training accuracy struggles to converge. What is the most probable reason for this behavior?

**Options:**

- A) Use Amazon S3's cross-region replication feature to duplicate the training dataset
- B) Switch to ReLU activation functions to mitigate the vanishing gradient problem
- C) Apply AWS Lambda's function layering to split the neural network into smaller tasks
- D) Increase the epoch count using Amazon SageMaker's training job configuration
- E) Implement data augmentation techniques to increase dataset diversity

**ì •ë‹µ: B) Switch to ReLU activation functions to mitigate the vanishing gradient problem**

ğŸ’¡ ì¶”ê°€ ì„¤ëª…:

- **Option A** - S3 ë³µì œëŠ” ë°ì´í„° ê°€ìš©ì„± í–¥ìƒì´ì§€ ëª¨ë¸ ìˆ˜ë ´ ë¬¸ì œ í•´ê²°ì±…ì´ ì•„ë‹˜
- **Option C** - Lambda í•¨ìˆ˜ ë ˆì´ì–´ë§ì€ ë°°í¬/ê´€ë¦¬ ìµœì í™”ì´ì§€ ì‹ ê²½ë§ í›ˆë ¨ ë¬¸ì œì™€ ë¬´ê´€
- **Option D** - ì—í¬í¬ ì¦ê°€ëŠ” ì–¸ë”í”¼íŒ… í•´ê²°ì±…ì´ì§€ë§Œ ê¸°ìš¸ê¸° ì†Œì‹¤ë¡œ ì¸í•œ ìˆ˜ë ´ ë¬¸ì œëŠ” í•´ê²° ë¶ˆê°€
- **Option E** - ë°ì´í„° ì¦ê°•ì€ ê³¼ì í•© ë°©ì§€ìš©ì´ì§€ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œì™€ ë¬´ê´€