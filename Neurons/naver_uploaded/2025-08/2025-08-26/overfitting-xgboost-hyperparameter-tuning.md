---

title: overfitting-xgboost-hyperparameter-tuning
created: 2025-08-19
modified: 2025-08-19
tags:
- algorithm/xgboost
- problem/overfitting
- technique/hyperparameter
- constraint/generalization
- usecase/credit-risk
aliases: ["xgboost-overfitting", "boosting-regularization", "hyperparameter-tuning"]

---

# XGBoost ì˜¤ë²„í”¼íŒ… ë°©ì§€ë¥¼ ìœ„í•œ íš¨ê³¼ì ì¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

XGBoost ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì—ì„œëŠ” ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ì§€ë§Œ ê²€ì¦ ë°ì´í„°ì—ì„œ ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì§€ëŠ” ì˜¤ë²„í”¼íŒ… ë¬¸ì œê°€ ë°œìƒí•œ ê²½ìš°, **gamma ì¦ê°€**ì™€ **subsample ê°ì†Œ**ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë‹¤.

## ğŸ“ ì„¤ëª…

### XGBoostê°€ ì˜¤ë²„í”¼íŒ…ì— ì·¨ì•½í•œ ì´ìœ 

XGBoostëŠ” ìˆœì°¨ì ìœ¼ë¡œ íŠ¸ë¦¬ë¥¼ ìƒì„±í•˜ë©´ì„œ ì´ì „ ëª¨ë¸ì˜ ì˜¤ë¥˜ë¥¼ ë³´ì •í•˜ëŠ” ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ë™ì¼í•œ í›ˆë ¨ ë°ì´í„°ë¥¼ ë°˜ë³µì ìœ¼ë¡œ í•™ìŠµí•˜ê²Œ ë˜ë©´, ë…¸ì´ì¦ˆë‚˜ íŠ¹ì´ê°’ê¹Œì§€ í•™ìŠµí•˜ì—¬ ì˜¤ë²„í”¼íŒ…ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### íš¨ê³¼ì ì¸ ì˜¤ë²„í”¼íŒ… ë°©ì§€ ì „ëµ

**1. Gamma íŒŒë¼ë¯¸í„° ì¦ê°€**
- ë¦¬í”„ ë…¸ë“œ ë¶„í• ì— í•„ìš”í•œ ìµœì†Œ ì†ì‹¤ ê°ì†ŒëŸ‰ì„ ì œì–´
- ê°’ì´ í´ìˆ˜ë¡ ë” ë³´ìˆ˜ì ì¸ ë¶„í• ë¡œ ëª¨ë¸ ë³µì¡ë„ ê°ì†Œ
- ì§ì ‘ì ì¸ ì •ê·œí™” íš¨ê³¼ ì œê³µ

**2. Subsample íŒŒë¼ë¯¸í„° ê°ì†Œ**
- ê° íŠ¸ë¦¬ í•™ìŠµ ì‹œ ì‚¬ìš©í•  ë°ì´í„° ìƒ˜í”Œ ë¹„ìœ¨ ì œí•œ
- ë§¤ë²ˆ ë‹¤ë¥¸ ë°ì´í„° ì„œë¸Œì…‹ìœ¼ë¡œ í•™ìŠµí•˜ì—¬ ë‹¤ì–‘ì„± ì¦ê°€
- ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜ì—ì„œ íŠ¹íˆ íš¨ê³¼ì 

### ì•„í‚¤í…ì²˜ í”Œë¡œìš°

```
Training Data (100%)
        â†“
    Subsample (80%)
        â†“
Tree 1 â† Random Sample A
        â†“
Tree 2 â† Random Sample B  
        â†“
Tree 3 â† Random Sample C
        â†“
    Gamma Control
  (Conservative Split)
        â†“
Final Ensemble Model
```

### Trade-offs ê³ ë ¤ì‚¬í•­

**Gamma ì¦ê°€ì˜ ì¥ì **:
- ì§ì ‘ì ì¸ ì •ê·œí™” íš¨ê³¼
- ëª¨ë¸ ë³µì¡ë„ ì œì–´
- í•´ì„ ê°€ëŠ¥ì„± í–¥ìƒ

**Gamma ì¦ê°€ì˜ ë‹¨ì **:
- ê³¼ë„í•˜ê²Œ ë†’ì´ë©´ ì–¸ë”í”¼íŒ… ìœ„í—˜
- ìµœì ê°’ ì°¾ê¸° ì–´ë ¤ì›€

**Subsample ê°ì†Œì˜ ì¥ì **:
- ì•™ìƒë¸” íš¨ê³¼ ê·¹ëŒ€í™”
- ê³„ì‚° ì†ë„ í–¥ìƒ
- ë¶€ìŠ¤íŒ…ì—ì„œ ê·¼ë³¸ì  ë‹¤ì–‘ì„± ì œê³µ

**Subsample ê°ì†Œì˜ ë‹¨ì **:
- ì •ë³´ ì†ì‹¤ ê°€ëŠ¥ì„±
- ë°ì´í„°ì…‹ì´ ì‘ì„ ë•Œ ë¶ˆì•ˆì •ì„±

## ğŸ” ì£¼ìš”ê°œë…

### í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¹„êµ

- **Gamma (min_split_loss)**: ë¶„í• ì„ ìœ„í•œ ìµœì†Œ ì†ì‹¤ ê°ì†ŒëŸ‰, ë†’ì„ìˆ˜ë¡ ë³´ìˆ˜ì 
- **Subsample**: ê° íŠ¸ë¦¬ í•™ìŠµìš© ìƒ˜í”Œ ë¹„ìœ¨, ë‚®ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ì¦ê°€
- **Eta (learning_rate)**: í•™ìŠµë¥ , ë‚®ì„ìˆ˜ë¡ ì²œì²œíˆ í•™ìŠµ
- **Max_depth**: íŠ¸ë¦¬ ìµœëŒ€ ê¹Šì´, ë‚®ì„ìˆ˜ë¡ ë‹¨ìˆœí•œ ëª¨ë¸

### XGBoost vs ë‹¤ë¥¸ ë°©ë²•ë“¤

**ì™œ Eta ê°ì†Œë³´ë‹¤ Subsampleì´ ë” íš¨ê³¼ì ì¸ê°€?**
- Eta: í•™ìŠµ ì†ë„ë§Œ ì¡°ì ˆ, ì—¬ì „íˆ ê°™ì€ ë°ì´í„° ë°˜ë³µ í•™ìŠµ
- Subsample: ê·¼ë³¸ì ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ ë‹¤ì–‘í™”í•˜ì—¬ ì˜¤ë²„í”¼íŒ… ë°©ì§€

### ì‹¤ì „ ì ìš©

- **ì‹ ìš©í‰ê°€ ëª¨ë¸**: subsample=0.8, gamma=2ë¡œ ì„¤ì •í•˜ì—¬ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
- **ì¶”ì²œ ì‹œìŠ¤í…œ**: colsample_bytreeì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ í”¼ì²˜ ë‹¤ì–‘ì„± í™•ë³´
- **ì‹œê³„ì—´ ì˜ˆì¸¡**: ì‹œê°„ì  íŒ¨í„´ ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ìƒ˜í”Œë§ ì „ëµ ì ìš©

## ğŸ“ ê´€ë ¨ ë¬¸ì œ

**Question:** A data science team at a fintech company has developed an XGBoost model to predict credit risk based on historical loan repayment data. While the model demonstrates high accuracy when tested against the training dataset, its performance significantly drops when evaluated using a separate validation dataset, indicating a potential overfitting issue. To address this problem and enhance the model's generalization to new, unseen data, which hyperparameter adjustment should the team consider as the most effective solution? (Choose TWO)

**Options:**

- A) Increase the 'gamma' parameter in XGBoost
- B) Decrease the 'colsample_bytree' parameter in XGBoost  
- C) Increase the 'max_depth' parameter in XGBoost
- D) Reduce the 'eta' parameter in XGBoost
- E) Decrease the 'subsample' parameter in XGBoost

**ì •ë‹µ: A) Increase gamma, E) Decrease subsample**

ğŸ’¡ ì¶”ê°€ ì„¤ëª…:

- **Option A (Increase gamma)** - ë¦¬í”„ ë…¸ë“œ ë¶„í• ì„ ë” ë³´ìˆ˜ì ìœ¼ë¡œ ë§Œë“¤ì–´ ëª¨ë¸ ë³µì¡ë„ ê°ì†Œ
- **Option E (Decrease subsample)** - ê° íŠ¸ë¦¬ë§ˆë‹¤ ë‹¤ë¥¸ ë°ì´í„°ë¡œ í•™ìŠµí•˜ì—¬ ì•™ìƒë¸” ë‹¤ì–‘ì„± ì¦ê°€  
- **Option B (Decrease colsample_bytree)** - í”¼ì²˜ ì„œë¸Œìƒ˜í”Œë§ë„ íš¨ê³¼ì ì´ì§€ë§Œ ìš°ì„ ìˆœìœ„ê°€ ë‚®ìŒ
- **Option C (Increase max_depth)** - íŠ¸ë¦¬ ê¹Šì´ ì¦ê°€ëŠ” ì˜¤íˆë ¤ ì˜¤ë²„í”¼íŒ… ì•…í™”
- **Option D (Reduce eta)** - í•™ìŠµë¥  ê°ì†ŒëŠ” ìˆ˜ë ´ ì•ˆì •ì„±ì€ ë†’ì´ì§€ë§Œ ì˜¤ë²„í”¼íŒ… í•´ê²°ì—ëŠ” ì œí•œì 