---
title: paraphrasing-blazingtext-wordembedding
created: 2025-08-16
modified: 2025-08-16
tags:
- service/blazingtext
- technique/word-embedding
- usecase/paraphrasing
- method/supervised
- feature/semantic-similarity
aliases: ["word-embedding", "blazingtext", "paraphrasing-tool"]
---

# BlazingText Word Embeddingì„ í™œìš©í•œ NLP íŒŒë¼í”„ë ˆì´ì§• ë„êµ¬ ì„±ëŠ¥ í–¥ìƒ

## ğŸ¯ í•µì‹¬ í¬ì¸íŠ¸

NLP íŒŒë¼í”„ë ˆì´ì§• ë„êµ¬ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë ¤ëŠ” ê²½ìš° AWS SageMaker BlazingTextì—ì„œ, ì‚¬ì „ í›ˆë ¨ëœ Word Embeddingì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ íƒœìŠ¤í¬ì˜ ì…ë ¥ í”¼ì²˜ë¡œ í™œìš©í•  ìˆ˜ ìˆë‹¤.

## ğŸ“ ì„¤ëª…

### BlazingTextê°€ íŒŒë¼í”„ë ˆì´ì§• ë„êµ¬ì— ì í•©í•œ ì´ìœ 

AWS SageMaker BlazingTextëŠ” Word2vecê³¼ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì•Œê³ ë¦¬ì¦˜ì˜ ìµœì í™”ëœ êµ¬í˜„ì²´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. Word embeddingì€ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ í‘œí˜„í•˜ì—¬ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì´ ë²¡í„° ê³µê°„ì—ì„œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ë°°ì¹˜ë˜ë„ë¡ í•©ë‹ˆë‹¤.

**í•µì‹¬ ì‘ë™ ë°©ì‹:**
- ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ë¡œ ì‚¬ì „ í›ˆë ¨ëœ word embedding í™œìš©
- ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ìˆ˜ì¹˜ì  ë²¡í„°ë¡œ í‘œí˜„
- ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë“±ì„ í†µí•´ ìœ ì‚¬í•œ ì˜ë¯¸ì˜ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ ì¶”ì²œ

**ê¸°ìˆ ì  êµ¬í˜„:**
```python
# ì‚¬ì „ í›ˆë ¨ëœ embedding ë¡œë“œ
embedding_matrix = load_pretrained_embeddings()

# íŒŒë¼í”„ë ˆì´ì§• ëª¨ë¸ì— embedding layer ì¶”ê°€
model = Sequential([
    Embedding(vocab_size, embedding_dim, 
             weights=[embedding_matrix],  # Word embedding feeding
             trainable=False),
    LSTM(128),
    Dense(vocab_size, activation='softmax')
])
```

### ì•„í‚¤í…ì²˜ í”Œë¡œìš°

```
1. ì‚¬ì „ í›ˆë ¨ëœ Word Embedding ë‹¤ìš´ë¡œë“œ
   BlazingText/Word2Vec â†’ "ì§‘":[0.2, -0.5, 0.8, ...]
   â†“
2. íŒŒë¼í”„ë ˆì´ì§• íˆ´ì— Word Embedding ì—°ê²°
   ì‚¬ìš©ì ì…ë ¥: "ë‚˜ëŠ” ì§‘ì— ê°„ë‹¤"
   â†“
3. ë‹¨ì–´ë³„ ë²¡í„° ë³€í™˜ ë° ë¶„ì„
   "ì§‘" â†’ embedding vector â†’ ìœ ì‚¬ë„ ê³„ì‚°
   â†“
4. ì˜ë¯¸ì  ìœ ì‚¬ ë‹¨ì–´ ì¶”ì²œ
   "ì§‘" â‰ˆ "ì£¼íƒ"(0.89), "ê°€ì˜¥"(0.85), "ê±°ì£¼ì§€"(0.81)
   â†“
5. íŒŒë¼í”„ë ˆì´ì§• ê²°ê³¼ ìƒì„±
   "ë‚˜ëŠ” ì£¼íƒì— ê°„ë‹¤", "ë‚˜ëŠ” ê°€ì˜¥ì— ê°„ë‹¤" ë“±
```

### Trade-offs ê³ ë ¤ì‚¬í•­

**Word Embedding ì¥ì **:
- ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ê´€ê³„ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í¬ì°©
- ì‚¬ì „ í›ˆë ¨ëœ ì§€ì‹ìœ¼ë¡œ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥
- ì œí•œëœ ë°ì´í„°ë¡œ í›ˆë ¨ë˜ëŠ” ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ
- ëŒ€ê·œëª¨ ì½”í¼ìŠ¤ì˜ ì–¸ì–´ ì§€ì‹ í™œìš©

**Word Embedding ë‹¨ì **:
- ë„ë©”ì¸ íŠ¹í™” ìš©ì–´ì— ëŒ€í•œ ì œí•œì  ì„±ëŠ¥
- ë¬¸ë§¥ì— ë”°ë¥¸ ë‹¤ì˜ì–´ ì²˜ë¦¬ì˜ ì–´ë ¤ì›€
- ì—…ë°ì´íŠ¸ëœ ì–¸ì–´ ì‚¬ìš©ë²• ë°˜ì˜ ì§€ì—°

**One-hot Encoding ì¥ì **:
- êµ¬í˜„ì´ ë‹¨ìˆœí•˜ê³  ì§ê´€ì 
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì˜ˆì¸¡ ê°€ëŠ¥

**One-hot Encoding ë‹¨ì **:
- ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ì „í˜€ í‘œí˜„í•˜ì§€ ëª»í•¨
- ê³ ì°¨ì› í¬ì†Œ ë²¡í„°ë¡œ ì¸í•œ ë¹„íš¨ìœ¨ì„±
- íŒŒë¼í”„ë ˆì´ì§•ì— í•„ìš”í•œ ìœ ì‚¬ì„± ì •ë³´ ë¶€ì¬

## ğŸ” ì£¼ìš”ê°œë…

### í•µì‹¬ ê°œë… ë¹„êµ

- **Word Embedding**: ë‹¨ì–´ë¥¼ ê³ ë°€ë„ ë²¡í„°ë¡œ í‘œí˜„í•˜ì—¬ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ìˆ˜ì¹˜ì ìœ¼ë¡œ ê³„ì‚° ê°€ëŠ¥í•œ ë°©ë²•
- **One-hot Encoding**: ë‹¨ì–´ë¥¼ í¬ì†Œ ì´ì§„ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ë‹¨ì–´ ê°„ ê´€ê³„ ì •ë³´ ì—†ìŒ
- **Levenshtein Distance**: ë¬¸ìì—´ ê°„ í¸ì§‘ ê±°ë¦¬ë¥¼ ì¸¡ì •í•˜ì—¬ ì² ìë²•ì  ìœ ì‚¬ì„±ë§Œ íŒŒì•…
- **BlazingText**: AWS SageMakerì˜ Word2vec ìµœì í™” êµ¬í˜„ì²´ë¡œ íš¨ìœ¨ì ì¸ word embedding ì œê³µ

### ì‹¤ì „ ì ìš©

- **ì½˜í…ì¸  ì‘ì„± ë„êµ¬**: ë¸”ë¡œê±°ë‚˜ ì‘ê°€ê°€ ë‹¤ì–‘í•œ í‘œí˜„ìœ¼ë¡œ ê¸€ì„ í’ë¶€í•˜ê²Œ ì‘ì„±
- **ë²ˆì—­ í’ˆì§ˆ ê°œì„ **: ë²ˆì—­ ì‹œ ë” ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€ì•ˆ í‘œí˜„ ì œê³µ
- **ê²€ìƒ‰ ì¿¼ë¦¬ í™•ì¥**: ì‚¬ìš©ì ê²€ìƒ‰ì–´ì™€ ì˜ë¯¸ì ìœ¼ë¡œ ìœ ì‚¬í•œ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰ ê²°ê³¼ í™•ì¥

## ğŸ“ ê´€ë ¨ ë¬¸ì œ

**Question:** A Machine Learning Specialist is developing an NLP model that powers a paraphrasing tool. This tool aims to generate closely associated words that users can pick to help construct sentences. The Specialist wants to boost model performance by feeding word features to a downstream task. How can the Specialist achieve this goal?

**Options:**
- A) Download a pre-trained word embedding
- B) Produce a vector of every word from a corpus and calculate the Levenshtein distance
- C) Use Amazon IQ to create a set of related words
- D) Transform a large text dataset using one-hot encoding

**ì •ë‹µ: A) Download a pre-trained word embedding**

ğŸ’¡ ì¶”ê°€ ì„¤ëª…:
- **B) Levenshtein distance** - í¸ì§‘ ê±°ë¦¬ëŠ” ì² ìë²•ì  ìœ ì‚¬ì„±ë§Œ ì¸¡ì •í•˜ì—¬ ì˜ë¯¸ì  ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ì—†ìŒ
- **C) Amazon IQ** - AWS ì „ë¬¸ê°€ ì°¾ê¸° í”Œë«í¼ìœ¼ë¡œ word embedding ê¸°ëŠ¥ê³¼ ë¬´ê´€í•¨
- **D) One-hot encoding** - ì´ì§„ ë²¡í„° í‘œí˜„ìœ¼ë¡œ ë‹¨ì–´ ê°„ ì˜ë¯¸ì  ê´€ê³„ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ ëª»í•¨